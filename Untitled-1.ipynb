{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING DATA PROCESSING + CLEANING\n",
      "\n",
      " DATA PROCESSING + CLEANING COMPLETE \n",
      "\n",
      "INITIALIZING DATA ANALYSIS\n",
      "The model being used is Valhalla, an optimized version of the Bart Large pretrained analysis model providing faster results. \n",
      " If you have not used it before, it may take a while to download as it is quite large.\n",
      "['racist', 'sexist', 'hatespeech']\n",
      "The labels entered were incorrect or incomplete, this process will now start over\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" candidate_labels = ['racist', 'sexist', 'hatespeech', 'neutral', 'offensive']\\ncandidate_results = [0, 0, 0, 0, 0]\\n\\nfor sent in tqdm(df_name['tweet'].values):\\n        \\n    res = classifier(sent, candidate_labels, multi_class = False) #change multiclass to True for different results\\n\\n    if res['labels'][0] == 'racist' and res['scores'][0] > 0.5:\\n        candidate_results[0] = candidate_results[0] + 1\\n    if res['labels'][0] == 'sexist' and res['scores'][0] > 0.5:\\n        candidate_results[1] = candidate_results[1] + 1\\n    if res['labels'][0] == 'hatespeech' and res['scores'][0] > 0.5:\\n        candidate_results[2] = candidate_results[2] + 1\\n    if res['labels'][0] == 'neutral' and res['scores'][0] > 0.5:\\n        candidate_results[3] = candidate_results[3] + 1\\n    if res['labels'][0] == 'offensive' and res['scores'][0] > 0.5:\\n        candidate_results[4] = candidate_results[4] + 1\\n\\n    if res['scores'][0] > 0.5:\\n        print(sent)\\n        print(res['labels'])\\n        print(res['scores'])\\n        print('\\n')\\n\\nprint(candidate_results) \""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import html\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "print('INITIALIZING DATA PROCESSING + CLEANING')\n",
    "\n",
    "try:\n",
    "    user_csv = input('Please input the exact name of the CSV file you wish to analyze: ')\n",
    "    tweet_column = input('Please input the name of the column containing the tweets: ')\n",
    "    tweet_column_with_quotes = \"'\" + tweet_column + \"'\"\n",
    "\n",
    "    dataframe = pd.read_csv(user_csv, delimiter=',',encoding='utf-8', header = 0)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    dataframe.rename(columns={tweet_column:'tweet'}) #renaming the tweet column to 'tweet'\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('There was an error finding the CSV you requested, please check the following:','\\n', '1. The CSV file is in the correct directory', '\\n', '2. You gave the correct name of the file, following the syntax: yourfilename.csv')\n",
    "\n",
    "\n",
    "df_copy = dataframe.copy() #creating a copy of the dataframe\n",
    "df_copy['tweet'] = df_copy['tweet'].str.lower() #making everything lower case\n",
    "df_copy.drop_duplicates(subset='tweet', keep='first', inplace=True, ignore_index=False) #removing duplicates\n",
    "df_copy[~df_copy.tweet.str.startswith('rt')] #removing retweets\n",
    "df_copy['tweet'] = df_copy['tweet'].apply(lambda k: html.unescape(str(k))) #removing unnecessary characters\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) #Removed mentions\n",
    "    text = re.sub(r'#', '', text) #Removed hashtags\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) #Remove the hyperlink\n",
    "    text = re.sub(r'\\'[\\s]+', '', text) #Remove apostrophe\n",
    "    text = re.sub(r'\\...+', '', text) #Remove dots\n",
    "    text = re.sub(r'\\!', '', text) #Remove exclamation  marks\n",
    "\n",
    "    return text\n",
    "\n",
    "df_copy['tweet'] = df_copy['tweet'].apply(clean_text)\n",
    "\n",
    "df_copy.to_csv('Cleaned_Data.csv')\n",
    "\n",
    "print('\\n','DATA PROCESSING + CLEANING COMPLETE', '\\n')\n",
    "print('INITIALIZING DATA ANALYSIS')\n",
    "\n",
    "print('The model being used is Valhalla, an optimized version of the Bart Large pretrained analysis model providing faster results.', '\\n', 'If you have not used it before, it may take a while to download as it is quite large.')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/distilbart-mnli-12-1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"valhalla/distilbart-mnli-12-1\", device = -1)\n",
    "\n",
    "df_original  = pd.read_csv(r'Cleaned_Data.csv')\n",
    "rows = df_original['tweet'].count()\n",
    "\n",
    "try:\n",
    "    number_of_rows = int(input(f'Please specify the number of rows you wish to analyze, in your current dataset, there are {rows} rows of tweets, if you wish to look at them all, input any non-numerical character'))\n",
    "    df_name = df_original.head(number_of_rows)\n",
    "except ValueError:\n",
    "    df_name = df_original\n",
    "\n",
    "labels = input('Please specify the lables you wish to analyze the tweets with, type \"done\" once you have inputted the ones you wish to use')\n",
    "\n",
    "candidate_labels = []\n",
    "candidate_results = []\n",
    "\n",
    "while labels != 'done':\n",
    "    candidate_labels.append(labels)\n",
    "    labels = input('Please specify the lables you wish to analyze the tweets with, type \"done\" once you have inputted the ones you wish to use')\n",
    "\n",
    "print(f'{candidate_labels}')\n",
    "approval = input('Are these labels correct? if so, type \"y\", if not, type \"n\"')\n",
    "\n",
    "if approval == 'y':\n",
    "    print(f'These are the labels which will be used: {candidate_labels}')\n",
    "elif approval == 'n':\n",
    "    print(f'The labels entered were incorrect or incomplete, this process will now start over')\n",
    "    candidate_labels = []\n",
    "    candidate_results = []\n",
    "    labels = input('Please specify the lables you wish to analyze the tweets with, type \"done\" once you have inputted the ones you wish to use')\n",
    "    while labels != 'done':\n",
    "        candidate_labels.append(labels)\n",
    "        labels = input('Please specify the lables you wish to analyze the tweets with, type \"done\" once you have inputted the ones you wish to use')\n",
    "        print(f'{candidate_labels}')\n",
    "        approval = input('Are these labels correct? if so, type \"y\", if not, type \"n\"')\n",
    "else:\n",
    "    print('You have inputted something other than \"y\" or \"n\", please input one of these')\n",
    "    print(f'{candidate_labels}')\n",
    "    approval = input('Are these labels correct? if so, type \"y\", if not, type \"n\"')\n",
    "\n",
    "for x in range(len(candidate_labels)):\n",
    "    candidate_results.append(0)\n",
    "\n",
    "print(candidate_results)\n",
    "\n",
    "\n",
    "\"\"\" candidate_labels = ['racist', 'sexist', 'hatespeech', 'neutral', 'offensive']\n",
    "candidate_results = [0, 0, 0, 0, 0]\n",
    "\n",
    "for sent in tqdm(df_name['tweet'].values):\n",
    "        \n",
    "    res = classifier(sent, candidate_labels, multi_class = False) #change multiclass to True for different results\n",
    "\n",
    "    if res['labels'][0] == 'racist' and res['scores'][0] > 0.5:\n",
    "        candidate_results[0] = candidate_results[0] + 1\n",
    "    if res['labels'][0] == 'sexist' and res['scores'][0] > 0.5:\n",
    "        candidate_results[1] = candidate_results[1] + 1\n",
    "    if res['labels'][0] == 'hatespeech' and res['scores'][0] > 0.5:\n",
    "        candidate_results[2] = candidate_results[2] + 1\n",
    "    if res['labels'][0] == 'neutral' and res['scores'][0] > 0.5:\n",
    "        candidate_results[3] = candidate_results[3] + 1\n",
    "    if res['labels'][0] == 'offensive' and res['scores'][0] > 0.5:\n",
    "        candidate_results[4] = candidate_results[4] + 1\n",
    "\n",
    "    if res['scores'][0] > 0.5:\n",
    "        print(sent)\n",
    "        print(res['labels'])\n",
    "        print(res['scores'])\n",
    "        print('\\n')\n",
    "\n",
    "print(candidate_results) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24740"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
